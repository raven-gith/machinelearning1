{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raven-gith/machinelearning1/blob/main/UTS/REGRESI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87IeVYFX0-M_",
        "outputId": "89ad8c36-af5a-44d4-a436-08fe994075a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.4.26)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "# ===== INSTALL & IMPORT LIBRARY =====\n",
        "!pip install gdown\n",
        "import gdown\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import VarianceThreshold, mutual_info_regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8MBCXX7-xv6",
        "outputId": "ef4b40dd-467c-46cf-b310-9ae2ccf93e19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1oEXSD137hPm_Fx5xSPRvlAr3silj3suK\n",
            "From (redirected): https://drive.google.com/uc?id=1oEXSD137hPm_Fx5xSPRvlAr3silj3suK&confirm=t&uuid=5f63e404-ea46-4a6b-9f97-fc882661a772\n",
            "To: /content/RegresiUTSTelkom.csv\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 443M/443M [00:08<00:00, 54.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "# ===== 1. PENGUMPULAN DATA (gdown) =====\n",
        "file_id = \"1oEXSD137hPm_Fx5xSPRvlAr3silj3suK\"\n",
        "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output=\"RegresiUTSTelkom.csv\", quiet=False)\n",
        "\n",
        "df = pd.read_csv(\"RegresiUTSTelkom.csv\")\n",
        "\n",
        "# ===== 2. PEMBERSIHAN DATA =====\n",
        "df = df.drop_duplicates()\n",
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68ORHKGD-0k3",
        "outputId": "c2af9ab9-e861-44f4-a586-4e4532ace4e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mutual Information Score (Top 10):\n",
            " 10.20556     0.140863\n",
            "68.40795     0.132764\n",
            "-27.46348    0.109744\n",
            "13.0162      0.094615\n",
            "179.19498    0.089067\n",
            "-7.26272     0.073053\n",
            "-0.36994     0.072543\n",
            "611.10913    0.069140\n",
            "-8.41558     0.068148\n",
            "6.02015      0.067854\n",
            "dtype: float64\n",
            "\n",
            "ANOVA dijelaskan secara teori karena dataset besar.\n"
          ]
        }
      ],
      "source": [
        "# ===== 3. FEATURE SELECTION =====\n",
        "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "target = numerical_cols[-1]\n",
        "\n",
        "# --- Constant / Quasi-Constant ---\n",
        "selector = VarianceThreshold(threshold=0.01)\n",
        "_ = selector.fit_transform(df[numerical_cols])\n",
        "\n",
        "# --- Correlation ---\n",
        "corr_matrix = df[numerical_cols].corr().abs()\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "to_drop_corr = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
        "df.drop(columns=to_drop_corr, inplace=True)\n",
        "\n",
        "# --- Mutual Information ---\n",
        "mi = mutual_info_regression(df[numerical_cols[:-1]], df[target])\n",
        "mi_series = pd.Series(mi, index=numerical_cols[:-1]).sort_values(ascending=False)\n",
        "print(\"\\nMutual Information Score (Top 10):\\n\", mi_series.head(10))\n",
        "\n",
        "# === ANOVA (Hanya Disebut di Laporan, Skip Eksekusi untuk Hemat RAM) ===\n",
        "print(\"\\nANOVA dijelaskan secara teori karena dataset besar.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZeUucZ3-320"
      },
      "outputs": [],
      "source": [
        "# ===== 4. FEATURE ENGINEERING =====\n",
        "df_encoded = df.copy()\n",
        "for col in categorical_cols:\n",
        "    df_encoded[col] = LabelEncoder().fit_transform(df_encoded[col].astype(str))\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_encoded[numerical_cols] = scaler.fit_transform(df_encoded[numerical_cols])\n",
        "\n",
        "# ===== 5. SPLIT DATA =====\n",
        "X = df_encoded.drop(columns=[target])\n",
        "y = df_encoded[target]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5n7umvZ-5bK"
      },
      "outputs": [],
      "source": [
        "# ===== 6. MODELING =====\n",
        "models = {\n",
        "    'LinearRegression': LinearRegression(),\n",
        "    'PolynomialRegression': Pipeline([('poly', PolynomialFeatures(degree=2)), ('linear', LinearRegression())]),\n",
        "    'DecisionTree': DecisionTreeRegressor(random_state=42),\n",
        "    'KNN': KNeighborsRegressor(),\n",
        "    'Bagging': BaggingRegressor(random_state=42),\n",
        "    'AdaBoost': AdaBoostRegressor(random_state=42),\n",
        "    'GradientBoosting': GradientBoostingRegressor(random_state=42),\n",
        "    'SVR': SVR()\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    results.append({'Model': name, 'MSE': mse, 'RMSE': rmse, 'R2': r2})\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\nHasil Evaluasi Model:\\n\", results_df.sort_values(by='R2', ascending=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEzFtFOs-7F0"
      },
      "outputs": [],
      "source": [
        "# ===== 7. VISUALISASI =====\n",
        "best_model_name = results_df.sort_values(by='R2', ascending=False).iloc[0]['Model']\n",
        "print(f\"\\nModel Terbaik: {best_model_name}\")\n",
        "\n",
        "best_model = models[best_model_name]\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(y_test, y_pred_best, alpha=0.7)\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title(f'Actual vs Predicted Values ({best_model_name})')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SOAL ANALISA\n",
        "\n",
        "\n",
        "\n",
        "1. Strategi Mengatasi Underfitting pada Linear Regression dan Decision Tree\n",
        "Underfitting terjadi ketika model terlalu sederhana sehingga tidak dapat menangkap pola dari data.\n",
        "\n",
        "Pendekatan A: Transformasi Fitur (Feature Engineering)\n",
        "\n",
        "Contoh: menambahkan fitur polinomial (misalnya xÂ², xÂ³) pada Linear Regression.\n",
        "\n",
        "Pengaruh terhadap bias-variance:\n",
        "\n",
        "Menurunkan bias karena model bisa mempelajari pola non-linear.\n",
        "\n",
        "Variance mungkin sedikit meningkat, tapi bisa dikontrol dengan regularisasi (Ridge/Lasso).\n",
        "\n",
        "Cocok jika data memiliki hubungan non-linear namun masih cukup \"teratur\".\n",
        "\n",
        "Pendekatan B: Ganti ke Model yang Lebih Kompleks\n",
        "\n",
        "Contoh: ganti Linear Regression â†’ Random Forest Regressor atau Gradient Boosting; Decision Tree â†’ tambah ensemble (Bagging/Boosting).\n",
        "\n",
        "Pengaruh terhadap bias-variance:\n",
        "\n",
        "Bias berkurang drastis karena model dapat menangkap kompleksitas data.\n",
        "\n",
        "Variance meningkat, namun ensemble seperti Bagging menstabilkan generalisasi.\n",
        "\n",
        "Perbandingan:\n",
        "\n",
        "Transformasi fitur efektif untuk model linear namun butuh eksplorasi manual.\n",
        "\n",
        "Model kompleks otomatis menangani pola kompleks tetapi perlu tuning dan komputasi lebih.\n",
        "\n",
        "2. Alternatif Loss Function untuk Regresi\n",
        "a. MAE (Mean Absolute Error)\n",
        "Keunggulan:\n",
        "\n",
        "Lebih tahan terhadap outlier karena penalti linier.\n",
        "\n",
        "Cocok jika kesalahan besar tidak harus dihukum keras.\n",
        "\n",
        "Kelemahan:\n",
        "\n",
        "Tidak terdiferensiasi di titik nol â†’ sulit untuk optimasi berbasis turunan (gradient-based methods).\n",
        "\n",
        "Cocok untuk: data dengan banyak outlier.\n",
        "\n",
        "b. Huber Loss\n",
        "Menggabungkan keunggulan MSE dan MAE: menggunakan MSE untuk kesalahan kecil dan MAE untuk kesalahan besar.\n",
        "\n",
        "Keunggulan:\n",
        "\n",
        "Lebih stabil untuk outlier dibanding MSE.\n",
        "\n",
        "Lebih mudah dioptimasi dibanding MAE.\n",
        "\n",
        "Kelemahan:\n",
        "\n",
        "Memiliki hyperparameter delta yang harus ditentukan.\n",
        "\n",
        "Cocok untuk: data dengan sebagian kecil outlier tapi tetap ingin mempertahankan sensitivitas terhadap error kecil.\n",
        "\n",
        "Kesimpulan:\n",
        "\n",
        "MSE â†’ cocok untuk distribusi normal & penalti besar.\n",
        "\n",
        "MAE â†’ robust terhadap outlier.\n",
        "\n",
        "Huber â†’ tradeoff fleksibel antara keduanya.\n",
        "\n",
        "3. Menilai Pentingnya Fitur Tanpa Mengetahui Nama\n",
        "a. Koefisien Regresi (Linear Model)\n",
        "Nilai absolut dari koefisien menunjukkan kontribusi relatif fitur (jika data sudah diskalakan).\n",
        "\n",
        "Keterbatasan:\n",
        "\n",
        "Hanya relevan jika fitur tidak berkorelasi (multikolinearitas merusak interpretasi).\n",
        "\n",
        "Tidak menangkap interaksi non-linear.\n",
        "\n",
        "b. Impurity-based Feature Importance (Tree-based Model)\n",
        "Mengukur berapa banyak impurity (MSE reduction) yang dikurangi oleh suatu fitur pada Decision Tree/Random Forest.\n",
        "\n",
        "Keterbatasan:\n",
        "\n",
        "Bias terhadap fitur dengan banyak nilai unik.\n",
        "\n",
        "Tidak stabil jika data terlalu kecil.\n",
        "\n",
        "c. Permutation Importance\n",
        "Ukur penurunan performa ketika satu fitur diacak â†’ cocok untuk model apa pun.\n",
        "\n",
        "Kelebihan: agnostik terhadap model.\n",
        "\n",
        "Kekurangan: mahal secara komputasi dan bisa terpengaruh oleh fitur berkorelasi.\n",
        "\n",
        "4. Desain Eksperimen untuk Tuning Hyperparameter\n",
        "Langkah Umum:\n",
        "Gunakan Grid Search atau Randomized Search dengan K-Fold Cross Validation.\n",
        "\n",
        "Contoh:\n",
        "\n",
        "max_depth untuk Decision Tree â†’ [3, 5, 10, 20, None]\n",
        "\n",
        "learning_rate untuk SGD â†’ log scale dari 1e-4 sampai 1e-1\n",
        "\n",
        "Trade-off Analysis:\n",
        "Aspek\tGrid Search\tRandomized Search\n",
        "Komputasi\tMahal, eksponensial jumlah kombinasi\tLebih hemat, cocok jika banyak parameter\n",
        "Stabilitas\tLebih stabil, tapi overkill jika range luas\tSedikit fluktuatif, tapi cukup akurat\n",
        "Generalisasi\tK-Fold CV penting untuk uji stabil\tHindari overfitting dengan hold-out/test set\n",
        "\n",
        "Gunakan Bayesian Optimization untuk efisiensi tinggi pada tuning kompleks.\n",
        "\n",
        "5. Langkah Jika Residual Plot Menunjukkan Non-linear & Heteroskedastisitas\n",
        "a. Tangani Non-Linearitas:\n",
        "Transformasi Fitur: polinomial, log, root, atau Box-Cox.\n",
        "\n",
        "Ganti Model: Gunakan model non-linear seperti Random Forest, Gradient Boosting, atau SVR.\n",
        "\n",
        "b. Tangani Heteroskedastisitas:\n",
        "Gunakan transformasi target (log, sqrt).\n",
        "\n",
        "Model alternatif seperti Generalized Linear Models (GLM).\n",
        "\n",
        "Gunakan Weighted Least Squares jika pola variansi bisa diestimasi.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "PENJELASAN SETIAP MODEL\n",
        "\n",
        "\n",
        "ðŸ“Š Analisis dan Model Terbaik\n",
        "Jika berdasarkan MSE, RÂ², dan hasil validasi (belum terlihat langsung karena bagian evaluasi belum diekstrak sepenuhnya), maka biasanya:\n",
        "\n",
        "Gradient Boosting Regressor cenderung terbaik dalam akurasi karena:\n",
        "\n",
        "Mengurangi bias dan variance secara bertahap.\n",
        "\n",
        "Bisa menangani hubungan non-linier kompleks.\n",
        "\n",
        "Cocok untuk data tabular seperti ini.\n",
        "\n",
        "Linear Regression atau KNN cenderung underfitting atau kurang stabil karena:\n",
        "\n",
        "Tidak mampu menangkap non-linearitas (Linear Regression).\n",
        "\n",
        "Sensitif terhadap distribusi dan outlier (KNN)."
      ],
      "metadata": {
        "id": "U_ULfSFLAfr2"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPoTp66pAoeeN8sc56W2uWu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}