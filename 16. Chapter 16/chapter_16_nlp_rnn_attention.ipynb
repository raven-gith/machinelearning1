{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raven-gith/machinelearning1/blob/main/16.%20Chapter%2016/chapter_16_nlp_rnn_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc4a9200",
      "metadata": {
        "id": "bc4a9200"
      },
      "source": [
        "# Chapter 16: Natural Language Processing with RNNs and Attention\n",
        "\n",
        "Notebook ini mereproduksi dan menjelaskan isi Bab 16 dari buku _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_ oleh AurÃ©lien GÃ©ron.\n",
        "\n",
        "## ğŸ“˜ Ringkasan Materi:\n",
        "\n",
        "Bab ini membahas penggunaan RNN dan attention mechanism untuk **Natural Language Processing (NLP)**.\n",
        "\n",
        "### Materi utama:\n",
        "1. Tokenisasi dan padding teks dengan `TextVectorization`\n",
        "2. Model NLP dengan Embedding + RNN\n",
        "3. Bidirectional LSTM untuk teks\n",
        "4. Attention Mechanism (konsep dasar)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "abc3b970",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abc3b970",
        "outputId": "720bb771-a4c0-4571-cd75-43d9bb2c7b1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[12  2 11  9  3  0]\n",
            " [10  3  7  8  0  0]\n",
            " [ 6  4  2  5  0  0]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# Contoh kalimat\n",
        "texts = [\"Aku suka belajar machine learning\", \"Deep learning sangat menarik\", \"Saya tidak suka spam\"]\n",
        "\n",
        "# Text vectorization layer\n",
        "vectorizer = keras.layers.TextVectorization(max_tokens=1000, output_sequence_length=6)\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(texts).batch(1)\n",
        "vectorizer.adapt(text_ds)\n",
        "\n",
        "# Cek hasil tokenisasi\n",
        "vectorized_texts = vectorizer(texts)\n",
        "print(vectorized_texts.numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c222e7f4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c222e7f4",
        "outputId": "2ed5f9d1-f621-477c-bcc7-01b496902a45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - accuracy: 0.6667 - loss: 0.6934\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - accuracy: 0.6667 - loss: 0.6910\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.6667 - loss: 0.6887\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.6667 - loss: 0.6864\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6667 - loss: 0.6841\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6667 - loss: 0.6818\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6667 - loss: 0.6795\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6667 - loss: 0.6771\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6667 - loss: 0.6747\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6667 - loss: 0.6723\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7937d6e13750>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "\n",
        "# Dataset dummy (vektorisasi output = binary sentiment)\n",
        "X = vectorized_texts.numpy()\n",
        "y = np.array([1, 1, 0])  # 1 = positif, 0 = negatif\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Embedding(input_dim=1000, output_dim=16, input_length=6),\n",
        "    keras.layers.Bidirectional(keras.layers.LSTM(16)),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.fit(X, y, epochs=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15dea2e4",
      "metadata": {
        "id": "15dea2e4"
      },
      "source": [
        "\n",
        "##  Konsep Attention (Dasar)\n",
        "\n",
        "**Attention mechanism** memungkinkan model memfokuskan perhatian ke bagian input yang paling relevan saat memproses sekuens panjang.  \n",
        "Secara umum, attention menghitung skor bobot dari setiap bagian input, lalu menggabungkannya secara dinamis berdasarkan konteks.\n",
        "\n",
        " Contoh: Digunakan di Transformer, BERT, dan banyak model NLP modern.\n",
        "\n",
        "Implementasi lengkap attention tidak dibahas di bab ini, tapi akan muncul di bab selanjutnya.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}